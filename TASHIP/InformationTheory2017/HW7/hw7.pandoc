---
title: Homework 7
author: Dilawar Singh
institute: NCBS Bangalore
email: dilawars@ncbs.res.in
geometry: right=5cm, marginparwidth=4cm
date : \today 
fontfamily: libertine
header-includes:
    - \usepackage{pgfplots}
    - \usepackage{libertine,mathpazo}
---

# Large deviation theory. 

We roll a fair six sided die $n$ times, where $n$ is large. What is the
probability that the number of 2s is at least double the number of 1s in the
resulting string?  Of strings which satisfy this condition, what proportion of
values will be 1s? 

We can split this calculation into the following parts:

a. What is the constraint equation for the set $E$ of types (or empirical
distributions) $P_x^n$ which satisfy the required condition?

b. Let $Q$ be the uniform distribution. Show that the member $P^*$ of E which
minimizes the relative entropy $D(P^*||Q)$ is the maximum entropy distribution
subject to the constraints from (a).

c. Find the distribution $P^*$. E.g. let the proportion of 1s be an unknown
$x$. Use some intuition to set the rest of the distribution. Then use calculus
to minimize.  You can find the answer analytically. 

d. What is the probability that the outcome of $n$ die rolls belongs to $P^*$
(and therefore, to $E$ first order in the exponent)?  

e. (Optional) Verify your answer for n = 250 by Monte Carlo simulation.

# Solution

Let's do the simulation first, just to feel the problem. We vary the value of
$n$ and compute the probability of these 'rare events' after recording 100 such
events.

\begin{tikzpicture}[scale=1 , every node/.style={} ]
    \begin{tikzpicture}[scale=1]
        \begin{semilogyaxis}[ xlabel=n,ylabel=p ]
        \addplot+ [color=blue] gnuplot [ raw gnuplot ] {
            plot "./results.txt" using (column("n")):(100/column("N")) with p;
        };
        \end{semilogyaxis}
    \end{tikzpicture}
\end{tikzpicture}
\begin{tikzpicture}[scale=1 , every node/.style={} ]
    \begin{tikzpicture}[scale=1]
        \begin{axis}[ xlabel=n,ylabel={Number of 1s} ]
        \addplot+ [color=blue] gnuplot [ raw gnuplot ] {
            plot "./results.txt" using (column("n")):(column("n1")) with p;
        };
        \end{axis}
    \end{tikzpicture}
\end{tikzpicture}    

The probability of rare events decreases logarithmically with $n$.

## Part a

The empirical distribution must have $\Pr(2) \ge \Pr(1)$. Or,

$$ 
\begin{aligned}
p_1 + p_2 + p_3 + p_4 + p_5 + p_6 &= 1 \\
p_2 & \ge 2 p_1 
\end{aligned}
$${#eq:eq1}

## Part b

$$D(P||Q) = \sum_i p_i \log\left( \frac{1}{6p_i} \right)$${#eq:eq2}

Find $P$ which minimizes $D(P||Q)$ subject to @eq:eq1. We use Langrange
multipliers. See the Wiki article for more details.

$$L(p1,p2,\ldots,p_6,\lambda_1,\lambda_2) = D(P||Q) + \lambda_1(p_2-p_1) + 
\lambda_2(p_1+p_2+\ldots+p_6)$$ 

We differentiate $L$ with respect to $p_1,p_2,\ldots,p_6,\lambda_1,\lambda_2$
and set to 0. We get the following equations.

$$
\begin{aligned}
1 + \log( 6p_1 ) - 2 \lambda_1 + \lambda_2 &= 0 \\
1 + \log( 6p_2 ) - \lambda_1 + \lambda_2 &= 0 \\
1 + \log( 6p_3 ) - 0 + \lambda_2 &= 0 \\
1 + \log( 6p_4 ) - 0 + \lambda_2 &= 0 \\
\vdots \\
1 + \log( 6p_6 ) - 0 + \lambda_2 &= 0 \\
p_2 &= 2 p_1 \\
p_1 + p_2 + \dots + p+6 &= 1 
\end{aligned}
$$

These leads to following (I am not wrong). $\frac{p_2}{p_3} = 2^{1/3}$,
$p_2=2p_1$ and $p_3=p_4=p_5=p_6$. 

$p_1=\frac{1}{3 + 4^{4/3}}, p_2=2p_1,p_3=p_4=p_5=p_6=\frac{2^{2/3}}{3+4^{4/3}}$.
According to these calculation distribution $H(E)=2.557196$ bits _which is very
close to numerical results we have in @fig:hvsd_. Note that we have only shown
that this minimizes $E$. \marginpar{It is not hard to show this also maximizes
$H$}

The simulation shows (@fig:hvsd)  for a distribution $E$ which satisfies the
condition above, $D(E||Q)$ decreases with $H(E)$. Now we need a proof of this.

![KL divergence Vs Entropy](./problem2.pdf){ #fig:hvsd }

## Part c

Assuming the $\Pr(1)=x$ then the $\Pr(2)=2x$ and
$\Pr(3)=\Pr(4)=\Pr(5)=\Pr(6)=\frac{1-3x}{4}$.

\begin{tikzpicture}[scale=1]
    \begin{axis}[ xlabel={$x=\Pr(1)$},ylabel=H(E) bits, grid, ymax=3 ]
    \addplot [color=blue] gnuplot [ raw gnuplot ] {
        H(x) = - x * log(x) / log(2) - 2*x * log(2*x)/log(2) 
            - 4* (1-3*x)/4 * log((1-3*x)/4)/log(2);
        plot [x=0.001:0.3] H(x);
    };
    \end{axis}
\end{tikzpicture}

## Part d


![KL divergence Vs Entropy](./problem2.pdf){#fig:hvsd}


